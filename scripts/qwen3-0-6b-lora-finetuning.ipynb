{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate datasets peft"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-12T05:10:22.962087Z",
          "iopub.execute_input": "2025-05-12T05:10:22.962368Z",
          "iopub.status.idle": "2025-05-12T05:10:26.249522Z",
          "shell.execute_reply.started": "2025-05-12T05:10:22.962347Z",
          "shell.execute_reply": "2025-05-12T05:10:26.248719Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load required libraries\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-12T05:10:35.381783Z",
          "iopub.execute_input": "2025-05-12T05:10:35.382426Z",
          "iopub.status.idle": "2025-05-12T05:10:35.386440Z",
          "shell.execute_reply.started": "2025-05-12T05:10:35.382399Z",
          "shell.execute_reply": "2025-05-12T05:10:35.385766Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load json dataset for finetuning \n",
        "encouraging_dataset = load_dataset(\"json\", data_files=\"/kaggle/input/personality-finetuning-inputs/career_encouraging.jsonl\", split=\"train\")\n",
        "critical_dataset = load_dataset(\"json\", data_files=\"/kaggle/input/personality-finetuning-inputs/career_critical.jsonl\", split=\"train\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7021b5e769c546ca80e171dfb5d18aee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f72ded82d49e4a19bb15a2f1ed9ff345"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 13,
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-12T05:10:39.658109Z",
          "iopub.execute_input": "2025-05-12T05:10:39.659003Z",
          "iopub.status.idle": "2025-05-12T05:10:40.075244Z",
          "shell.execute_reply.started": "2025-05-12T05:10:39.658973Z",
          "shell.execute_reply": "2025-05-12T05:10:40.074503Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and define format prompt \n",
        "base_model_name = \"Qwen/Qwen3-0.6B-Base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
        "\n",
        "def format_prompt(example, tokenizer):\n",
        "    full_prompt = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['response']}\"\n",
        "    tokenized = tokenizer(\n",
        "        full_prompt,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "    )\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"]\n",
        "    return tokenized"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/9.68k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d6e9e59cc40047c8a20f403778863b1f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3f4d42b1f54a4a449b1612a031aa4ef8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3190f17aa84b49f580649d2c4fccffd3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7d91cf09721240e6bc496113f5904da9"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 15,
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-12T05:10:59.218675Z",
          "iopub.execute_input": "2025-05-12T05:10:59.218963Z",
          "iopub.status.idle": "2025-05-12T05:11:00.765557Z",
          "shell.execute_reply.started": "2025-05-12T05:10:59.218942Z",
          "shell.execute_reply": "2025-05-12T05:11:00.764996Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the encouraging dataset\n",
        "tokenized_encouraging_dataset = encouraging_dataset.map(\n",
        "    format_prompt,\n",
        "    fn_kwargs={\"tokenizer\": tokenizer},\n",
        "    remove_columns=encouraging_dataset.column_names\n",
        ")\n",
        "\n",
        "# Tokenize the critical dataset\n",
        "tokenized_critical_dataset = critical_dataset.map(\n",
        "    format_prompt,\n",
        "    fn_kwargs={\"tokenizer\": tokenizer},\n",
        "    remove_columns=critical_dataset.column_names\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/10 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed470673902445998fffd8dd63cccf8c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/10 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a40d798285f74e9fb1a469444cea369f"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 16,
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-12T05:11:12.564996Z",
          "iopub.execute_input": "2025-05-12T05:11:12.565705Z",
          "iopub.status.idle": "2025-05-12T05:11:12.834299Z",
          "shell.execute_reply.started": "2025-05-12T05:11:12.565673Z",
          "shell.execute_reply": "2025-05-12T05:11:12.833390Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure trainer and train \n",
        "\n",
        "def train_lora_model(base_model_name, tokenized_dataset, persona_label):\n",
        "\n",
        "    print(f\"Training model for persona: {persona_label.upper()}\")\n",
        "\n",
        "    # Define save path\n",
        "    output_dir = f\"/kaggle/working/qwen-lora-career-{persona_label}\"\n",
        "    \n",
        "    # Load base model\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_name,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    \n",
        "    # Disable cache for training \n",
        "    model.config.use_cache = False \n",
        "\n",
        "    # Config lora \n",
        "    lora_config = LoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        target_modules=[\"q_proj\", \"v_proj\"]\n",
        "    )\n",
        "\n",
        "    # Apply lora \n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "    # Print trainable params \n",
        "    print(f\"Trainable parmams for persona: {persona_label.upper()}\")\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    # Training config \n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=8,\n",
        "        learning_rate=2e-4,\n",
        "        num_train_epochs=3,\n",
        "        logging_steps=10,\n",
        "        fp16=True,\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=1,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    # Trainer \n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    \n",
        "    # Save adapter and tokenizer\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    print(f\"Saved model for persona: {persona_label.upper()}\")"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-12T05:11:33.625016Z",
          "iopub.execute_input": "2025-05-12T05:11:33.625357Z",
          "iopub.status.idle": "2025-05-12T05:11:33.632321Z",
          "shell.execute_reply.started": "2025-05-12T05:11:33.625331Z",
          "shell.execute_reply": "2025-05-12T05:11:33.631606Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lora finetuning for encouraing and critical personas \n",
        "train_lora_model(base_model_name = base_model_name,\n",
        "                 tokenized_dataset = tokenized_encouraging_dataset, \n",
        "                 persona_label = \"encouraging\")\n",
        "train_lora_model(base_model_name = base_model_name,\n",
        "                 tokenized_dataset = tokenized_critical_dataset, \n",
        "                 persona_label = \"critical\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-12T05:12:48.122406Z",
          "iopub.execute_input": "2025-05-12T05:12:48.122743Z",
          "iopub.status.idle": "2025-05-12T05:13:08.803075Z",
          "shell.execute_reply.started": "2025-05-12T05:12:48.122720Z",
          "shell.execute_reply": "2025-05-12T05:13:08.802041Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data compression \n",
        "import shutil\n",
        "\n",
        "# Compress the trained encouraging model\n",
        "shutil.make_archive(\"/kaggle/working/qwen-lora-career-encouraging\", 'zip', \"/kaggle/working/qwen-lora-career-encouraging\")\n",
        "\n",
        "# Compress the trained critical model\n",
        "shutil.make_archive(\"/kaggle/working/qwen-lora-career-critical\", 'zip', \"/kaggle/working/qwen-lora-career-critical\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-12T05:13:26.325731Z",
          "iopub.execute_input": "2025-05-12T05:13:26.326031Z",
          "iopub.status.idle": "2025-05-12T05:13:30.915684Z",
          "shell.execute_reply.started": "2025-05-12T05:13:26.326007Z",
          "shell.execute_reply": "2025-05-12T05:13:30.915077Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import FileLink\n",
        "\n",
        "# Create download links\n",
        "display(FileLink('qwen-lora-career-encouraging.zip'))\n",
        "display(FileLink('qwen-lora-career-critical.zip'))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "/kaggle/working/qwen-lora-career-encouraging.zip",
            "text/html": "<a href='qwen-lora-career-encouraging.zip' target='_blank'>qwen-lora-career-encouraging.zip</a><br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "/kaggle/working/qwen-lora-career-critical.zip",
            "text/html": "<a href='qwen-lora-career-critical.zip' target='_blank'>qwen-lora-career-critical.zip</a><br>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 21,
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-12T05:16:23.694098Z",
          "iopub.execute_input": "2025-05-12T05:16:23.695071Z",
          "iopub.status.idle": "2025-05-12T05:16:23.703092Z",
          "shell.execute_reply.started": "2025-05-12T05:16:23.695033Z",
          "shell.execute_reply": "2025-05-12T05:16:23.702383Z"
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 11765288,
          "sourceType": "datasetVersion",
          "datasetId": 7386121
        }
      ],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "nteract": {
      "version": "0.28.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}